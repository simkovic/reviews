Review tools for academic reports of psychological research
=========================

If you work in academia you will be asked to referee papers. The format, tone, and content of reviews is usually learned via an apprenticeship model.  The goal is to avoid to provide helpful criticism to the authors and to prevent publication of flawed experiments, unjustified claims and poorly written reports. It is a responsibility that must be taken seriously, but almost everyone wishes other people did better. This feeling reflects the low validity of reviews found by the few academic studies that evalueted the review process. 

In principle, journals provide instructions to authors that should constrain the variability and subjectivity of the reviews. In practice, these instructions are ignored because there are many journals and it is time-consuming for the reviewer to read the often lengthy instruction everytime the she does his job. Furthermore, these instructions are often imprecise and occasionally include items where the reviewer judgments are unreliable and decrease the validity of the review as a whole. An example of such item is the impact of the research.

This repository provides tools that should help researchers to write better (more reliable and valid) reviews with less effort. A simple way to do this is to avoid asking reviewers to judge impact or inovativeness of the research and instead provide a checklist with most common issues that are easy to spot. These are mostly statistical and methodological issues that should be dead-obvious to people with statistical expertise. Unfortunately, a quick glance at the checklist should convince that most of the published psychological research does not satisfy these criteria and would not pass the review process without a revision. 

The repository includes the following documents:

* Review Instructions (adapted from Jeff Leek)
* Review Template 
* Checklist with most common issues
